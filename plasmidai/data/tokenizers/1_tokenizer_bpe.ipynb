{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LETTER_TO_BASES = {\n",
    "    \"A\": \"A\",\n",
    "    \"B\": \"CGT\",\n",
    "    \"C\": \"C\",\n",
    "    \"D\": \"AGT\",\n",
    "    \"G\": \"G\",\n",
    "    \"H\": \"ACT\",\n",
    "    \"K\": \"GT\",\n",
    "    \"M\": \"AC\",\n",
    "    \"N\": \"ACGT\",\n",
    "    \"R\": \"AG\",\n",
    "    \"S\": \"CG\",\n",
    "    \"T\": \"T\",\n",
    "    \"V\": \"ACG\",\n",
    "    \"W\": \"AT\",\n",
    "    \"Y\": \"CT\",\n",
    "}\n",
    "\n",
    "ROOT = 'e:/PlasmidAI' # '/scratch/adibvafa/plasmid-ai/'\n",
    "DATA_ROOT = f'{ROOT}/data'\n",
    "DATA_SPLITS = f'{DATA_ROOT}/splits.csv'\n",
    "BASE_FILE = 'plasmids_replicon'\n",
    "DATASET = f'{DATA_ROOT}/{BASE_FILE}.fasta'\n",
    "DATASET_TXT = f'{DATA_ROOT}/{BASE_FILE}.txt'\n",
    "DATASET_CUTOFF = f'{DATA_ROOT}/{BASE_FILE}_cutoff.txt'\n",
    "DATASET_DUMMY =f'{DATA_ROOT}/{BASE_FILE}_dummy.txt'\n",
    "DATASET_FINETUNE = f'{DATA_ROOT}/{BASE_FILE}_finetune.txt'\n",
    "DATASET_CUTOFF_RC = f'{DATA_ROOT}/{BASE_FILE}_cutoff_rc.txt'\n",
    "TOKENIZER = 'dna_bpe_tokenizer_offset.json'\n",
    "\n",
    "SEED = 42\n",
    "LEN_CUTOFF = 100_000\n",
    "VOCAB_SIZE = 4096\n",
    "NUM_SEQUENCES = 10     #54646\n",
    "MAX_TOKEN_LENGTH = 32\n",
    "SPECIAL_TOKENS = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_fasta_to_dataframe(file_path):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append([record.id, str(record.seq), record.description])\n",
    "    \n",
    "    dataset = pd.DataFrame(sequences, columns=['ID', 'Sequence', 'Description'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocess_dna_seqeunce(seq):\n",
    "    # Clean all whitespaces\n",
    "    seq = seq.upper()\n",
    "    cleaned_seq = seq.replace(' ', '').replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "    \n",
    "    # Replace each letter with a random base from LETTER_TO_BASES using random.randint\n",
    "    replaced_seq = ''\n",
    "    for letter in cleaned_seq:\n",
    "        bases = LETTER_TO_BASES[letter]\n",
    "        random_base = bases[random.randint(0, len(bases) - 1)]\n",
    "        replaced_seq += random_base\n",
    "    \n",
    "    return replaced_seq\n",
    "\n",
    "\n",
    "def reverse_compliment(dna):\n",
    "    return dna.translate(str.maketrans(\"ATCG\", \"TAGC\"))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read plasmid dataset and splits\n",
    "dataset = read_fasta_to_dataframe(DATASET)\n",
    "# dataset['Sequence'] = dataset['Sequence'].apply(preprocess_dna_seqeunce)\n",
    "\n",
    "# Select the finetune dataset\n",
    "data_splits = pd.read_csv(DATA_SPLITS)\n",
    "finetune_split = data_splits[(data_splits['split'] == 'train') & (data_splits['finetune'] == 1)]\n",
    "finetune_dataset = dataset[dataset['ID'].isin(finetune_split['id'])]\n",
    "\n",
    "# Select the cutoff dataset, whose length is less than LEN_CUTOFF\n",
    "cutoff_dataset = dataset[dataset['Sequence'].apply(lambda x: len(x) < LEN_CUTOFF)]\n",
    "\n",
    "# Add reverse compliment to the dataset\n",
    "cutoff_dataset['Sequence'] = cutoff_dataset['Sequence'].apply(preprocess_dna_seqeunce)\n",
    "reverse_compliment_dataset = cutoff_dataset.copy()\n",
    "reverse_compliment_dataset['Sequence'] = reverse_compliment_dataset['Sequence'].apply(reverse_compliment)\n",
    "cutoff_dataset_with_reverse_compliment = pd.concat([cutoff_dataset, reverse_compliment_dataset], axis=0)\n",
    "\n",
    "# Save dataset to txt for tokenizer\n",
    "dataset['Sequence'].to_csv(DATASET_TXT, index=False, header=False)\n",
    "# cutoff_dataset['Sequence'].to_csv(DATASET_CUTOFF, index=False, header=False)\n",
    "# finetune_dataset['Sequence'].to_csv(DATASET_FINETUNE, index=False, header=False)\n",
    "# dataset['Sequence'].iloc[:NUM_SEQUENCES].to_csv(DATASET_DUMMY, index=False, header=False)\n",
    "cutoff_dataset_with_reverse_compliment['Sequence'].to_csv(DATASET_CUTOFF_RC, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the SentencePiece tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=TOKENIZER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test the tokenizer on a sample DNA sequence\n",
    "sequence = \"ATTCTGCGGTTCCCCCTGGAAGACCTACGCAAGTTGGGCCAGCTCAGAGGTGGAATCAACGAAGGCGAGC\"\n",
    "encoded = tokenizer(sequence)\n",
    "print(\"Encoded sequence:\", encoded)\n",
    "\n",
    "# Decode the tokens back to the original sequence\n",
    "decoded_sequence = tokenizer.decode(encoded['input_ids'])\n",
    "print(\"Decoded sequence:\", decoded_sequence.upper())\n",
    "\n",
    "# Anlyze the vocabulary\n",
    "print(f'Alphabete: {set(''.join(list(sorted(tokenizer.vocab.keys())[::-1])[5:]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted(tokenizer.vocab.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
